{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets accelerate\n"
      ],
      "metadata": {
        "id": "MKDaJVNoL4vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWMxkoJ8Z7kM"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "7lDZjo9aaY4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "It is a truth universally acknowledged, that a single man in possession of a good fortune,\n",
        "must be in want of a wife. However little known the feelings or views of such a man may be\n",
        "on his first entering a neighbourhood, this truth is so well fixed in the minds of the\n",
        "surrounding families, that he is considered the rightful property of some one or other of\n",
        "their daughters.\n",
        "\n",
        "‚ÄúMy dear Mr. Bennet,‚Äù said his lady to him one day, ‚Äúhave you heard that Netherfield Park\n",
        "is let at last?‚Äù Mr. Bennet replied that he had not. ‚ÄúBut it is,‚Äù returned she; ‚Äúfor Mrs.\n",
        "Long has just been here, and she told me all about it.‚Äù Mr. Bennet made no answer.\n",
        "\n",
        "‚ÄúDo you not want to know who has taken it?‚Äù cried his wife impatiently.\n",
        "‚ÄúYou want to tell me, and I have no objection to hearing it.‚Äù This was invitation enough.\n",
        "\n",
        "Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large\n",
        "fortune from the north of England; that he came down on Monday in a chaise and four to see\n",
        "the place, and was so much delighted with it, that he agreed with Mr. Morris immediately;\n",
        "that he is to take possession before Michaelmas, and some of his servants are to be in the\n",
        "house by the end of next week.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "summarizer= pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\"\n",
        ")\n",
        "\n",
        "summary=summarizer(\n",
        "    long_text,\n",
        "    max_length=130,\n",
        "    min_length=40,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "print(\"SUMMARY:\\n\")\n",
        "print(summary[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "XJXigc2Ba413"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset= load_dataset(\"tomaarsen/setfit-absa-semeval-restaurants\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "iLEW6L2tfExc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample=dataset[\"train\"][0]\n",
        "sample"
      ],
      "metadata": {
        "id": "Xdp_AaTqfc0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "   print(dataset[\"train\"][i])\n",
        "   print(\"-\"*30)"
      ],
      "metadata": {
        "id": "vwYqel2bhwHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "\n",
        "dataset = load_dataset(\"tomaarsen/setfit-absa-semeval-restaurants\")\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "label2id={\n",
        "    \"O\":0,\n",
        "    \"B-ASP\":1,\n",
        "    \"I-ASP\":2\n",
        "}\n",
        "id2label = {v: k for k, v in label2id.items()}#basically the reverse of label2id\n",
        "\n",
        "# ========== FIX: Aggregate all aspects for each unique text ==========\n",
        "def aggregate_aspects(dataset_split):\n",
        "    \"\"\"Group all aspects by their text to avoid conflicting labels\"\"\"\n",
        "    text_to_aspects = defaultdict(list)\n",
        "    for example in dataset_split:\n",
        "        text_to_aspects[example[\"text\"]].append(example[\"span\"])\n",
        "\n",
        "    # Create new dataset with aggregated aspects\n",
        "    aggregated = []\n",
        "    for text, aspects in text_to_aspects.items():\n",
        "        aggregated.append({\"text\": text, \"aspects\": list(set(aspects))})  # remove duplicates\n",
        "    return Dataset.from_list(aggregated)\n",
        "\n",
        "train_dataset = aggregate_aspects(dataset[\"train\"])\n",
        "test_dataset = aggregate_aspects(dataset[\"test\"])\n",
        "\n",
        "def tokenize_and_align_labels(example):\n",
        "    text = example[\"text\"]\n",
        "    aspects = example[\"aspects\"]  # Now a LIST of aspects\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    offsets = tokenized[\"offset_mapping\"]\n",
        "    labels = [label2id[\"O\"]] * len(offsets)  # Initialize all as O\n",
        "\n",
        "    # Mark special tokens\n",
        "    for i, (start, end) in enumerate(offsets):\n",
        "        if start == end:\n",
        "            labels[i] = -100\n",
        "\n",
        "    # ========== FIX: Label ALL aspects in the text ==========\n",
        "    for aspect in aspects:\n",
        "        aspect_start = text.find(aspect)\n",
        "        if aspect_start == -1:\n",
        "            continue\n",
        "        aspect_end = aspect_start + len(aspect)\n",
        "\n",
        "        is_first_token = True\n",
        "        for i, (start, end) in enumerate(offsets):\n",
        "            if start == end:  # Skip special tokens\n",
        "                continue\n",
        "            # Token overlaps with aspect span\n",
        "            if start >= aspect_start and end <= aspect_end:\n",
        "                if is_first_token:\n",
        "                    labels[i] = label2id[\"B-ASP\"]\n",
        "                    is_first_token = False\n",
        "                else:\n",
        "                    labels[i] = label2id[\"I-ASP\"]\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    tokenized.pop(\"offset_mapping\")  # model doesn't need this\n",
        "    return tokenized\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_and_align_labels)\n",
        "test_dataset = test_dataset.map(tokenize_and_align_labels)\n",
        "\n",
        "train_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "\n",
        "test_dataset.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import BertForTokenClassification\n",
        "\n",
        "#loading bert for token classification\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3,          #output labels per token\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./aspect_model\",          # where checkpoints & final model go\n",
        "    eval_strategy=\"epoch\",           # evaluate after each epoch\n",
        "    learning_rate=2e-5,                    # standard BERT fine-tuning LR\n",
        "    per_device_train_batch_size=16,        # training batch size\n",
        "    per_device_eval_batch_size=16,         # evaluation batch size\n",
        "    num_train_epochs=3,                    # how many full passes over training data\n",
        "    weight_decay=0.01,                     # regularization\n",
        "    logging_steps=50,                      # log training loss every 50 steps\n",
        "    save_strategy=\"epoch\",                 # save model after each epoch\n",
        "    load_best_model_at_end=True            # keep best checkpoint (based on eval loss)\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./aspect_model\")\n",
        "tokenizer.save_pretrained(\"./aspect_model\")"
      ],
      "metadata": {
        "id": "Q30T4k97hzN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "Nuam0lK6Mxmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TrainingArguments)\n"
      ],
      "metadata": {
        "id": "Pu2vfjbnNNcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer from the folder you saved\n",
        "model = BertForTokenClassification.from_pretrained(\"./aspect_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./aspect_model\")\n",
        "\n",
        "# Put model in eval mode (no gradient calculation needed)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "5MtW9LCwSQ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_aspects(text):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "\n",
        "    # Get predictions (logits)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits  # shape: [batch_size, seq_len, num_labels]\n",
        "\n",
        "    # Get predicted label ids\n",
        "    predictions = torch.argmax(logits, dim=2)  # shape: [batch_size, seq_len]\n",
        "\n",
        "    # Convert ids to labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    labels = [model.config.id2label[pred.item()] for pred in predictions[0]]\n",
        "\n",
        "    # Collect aspects (FIXED: proper wordpiece handling)\n",
        "    aspects = []\n",
        "    current_aspect = \"\"\n",
        "    for token, label in zip(tokens, labels):\n",
        "        # Skip special tokens\n",
        "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "\n",
        "        if label == \"B-ASP\":\n",
        "            if current_aspect:\n",
        "                aspects.append(current_aspect.strip())\n",
        "            # Start new aspect\n",
        "            current_aspect = token.replace(\"##\", \"\")\n",
        "        elif label == \"I-ASP\":\n",
        "            # Continue aspect - no space if it's a subword (##)\n",
        "            if token.startswith(\"##\"):\n",
        "                current_aspect += token.replace(\"##\", \"\")\n",
        "            else:\n",
        "                current_aspect += \" \" + token\n",
        "        else:  # O label\n",
        "            if current_aspect:\n",
        "                aspects.append(current_aspect.strip())\n",
        "                current_aspect = \"\"\n",
        "\n",
        "    if current_aspect:\n",
        "        aspects.append(current_aspect.strip())\n",
        "\n",
        "    return aspects"
      ],
      "metadata": {
        "id": "cV9N9IxwSTJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The sushi was fresh and the drinks were cold.\"\n",
        "extracted_aspects = extract_aspects(text)\n",
        "print(\"Extracted Aspects:\", extracted_aspects)\n"
      ],
      "metadata": {
        "id": "9IHbv_7pSXA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment model\n",
        "\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"tomaarsen/setfit-absa-semeval-restaurants\")\n",
        "\n",
        "# ============================================\n",
        "# FIXED: Better data preparation\n",
        "# ============================================\n",
        "def prepare_sentiment_data(dataset_split):\n",
        "    \"\"\"Convert to sentiment classification with BETTER formatting\"\"\"\n",
        "    examples = []\n",
        "    skipped = 0\n",
        "\n",
        "    for item in dataset_split:\n",
        "        # Skip if label is invalid\n",
        "        if not item['label'] or item['label'] not in [\"positive\", \"neutral\", \"negative\", \"conflict\"]:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # FIXED: Use full sentence as input (aspect is already in the text!)\n",
        "        aspect = item['span']\n",
        "        text = item['text']\n",
        "        input_text = f\"{aspect}: {text}\"\n",
        "\n",
        "        # Map labels to ids\n",
        "        label_map = {\n",
        "            \"positive\": 2,\n",
        "            \"neutral\": 1,\n",
        "            \"negative\": 0,\n",
        "            \"conflict\": 1  # Treat conflict as neutral\n",
        "        }\n",
        "        label = label_map[item['label']]\n",
        "\n",
        "        examples.append({\n",
        "            \"text\": input_text,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "    print(f\"  Kept: {len(examples)} examples\")\n",
        "    if skipped > 0:\n",
        "        print(f\"  Skipped: {skipped} examples (invalid labels)\")\n",
        "\n",
        "    return Dataset.from_list(examples)\n",
        "\n",
        "# Process training data\n",
        "print(\"Processing training data...\")\n",
        "full_train = prepare_sentiment_data(dataset[\"train\"])\n",
        "\n",
        "# Split into train (80%) and validation (20%)\n",
        "print(\"\\nSplitting into train/validation...\")\n",
        "train_test_split = full_train.train_test_split(test_size=0.2, seed=42)\n",
        "train_sentiment = train_test_split[\"train\"]\n",
        "test_sentiment = train_test_split[\"test\"]\n",
        "\n",
        "print(f\"\\nFinal split:\")\n",
        "print(f\"  Training: {len(train_sentiment)} examples\")\n",
        "print(f\"  Validation: {len(test_sentiment)} examples\")\n",
        "\n",
        "# Check label distribution\n",
        "print(\"\\nLabel distribution in training set:\")\n",
        "label_counts = {}\n",
        "for example in train_sentiment:\n",
        "    label = example['label']\n",
        "    label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "for label_id, count in sorted(label_counts.items()):\n",
        "    label_name = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}[label_id]\n",
        "    print(f\"  {label_name}: {count} ({count/len(train_sentiment)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nSample:\", train_sentiment[0])\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return sentiment_tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing datasets...\")\n",
        "train_sentiment = train_sentiment.map(tokenize_function, batched=True)\n",
        "test_sentiment = test_sentiment.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_sentiment.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_sentiment.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Initialize sentiment model\n",
        "print(\"Loading BERT model...\")\n",
        "sentiment_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3,\n",
        "    id2label={0: \"negative\", 1: \"neutral\", 2: \"positive\"},\n",
        "    label2id={\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        ")\n",
        "\n",
        "# BETTER Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sentiment_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,  # One more epoch\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    warmup_steps=100  # Added warmup\n",
        ")\n",
        "\n",
        "# Train sentiment model\n",
        "trainer = Trainer(\n",
        "    model=sentiment_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sentiment,\n",
        "    eval_dataset=test_sentiment,\n",
        "    tokenizer=sentiment_tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING SENTIMENT CLASSIFIER\")\n",
        "print(\"=\"*60)\n",
        "trainer.train()\n",
        "\n",
        "# Save sentiment model\n",
        "print(\"\\nSaving model...\")\n",
        "trainer.save_model(\"./sentiment_model\")\n",
        "sentiment_tokenizer.save_pretrained(\"./sentiment_model\")\n",
        "print(\"‚úì Sentiment model saved to ./sentiment_model/\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "ihntOxRviJ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, BertForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load models\n",
        "print(\"Loading models...\")\n",
        "aspect_model = BertForTokenClassification.from_pretrained(\"./aspect_model\")\n",
        "aspect_tokenizer = AutoTokenizer.from_pretrained(\"./aspect_model\")\n",
        "sentiment_model = BertForSequenceClassification.from_pretrained(\"./sentiment_model\")\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"./sentiment_model\")\n",
        "\n",
        "aspect_model.eval()\n",
        "sentiment_model.eval()\n",
        "print(\"‚úì Models loaded!\\n\")\n",
        "\n",
        "# Extract aspects function\n",
        "def extract_aspects(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    labels = [model.config.id2label[pred.item()] for pred in predictions[0]]\n",
        "\n",
        "    aspects = []\n",
        "    current_aspect = \"\"\n",
        "    for token, label in zip(tokens, labels):\n",
        "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "        if label == \"B-ASP\":\n",
        "            if current_aspect:\n",
        "                aspects.append(current_aspect.strip())\n",
        "            current_aspect = token.replace(\"##\", \"\")\n",
        "        elif label == \"I-ASP\":\n",
        "            if token.startswith(\"##\"):\n",
        "                current_aspect += token.replace(\"##\", \"\")\n",
        "            else:\n",
        "                current_aspect += \" \" + token\n",
        "        else:\n",
        "            if current_aspect:\n",
        "                aspects.append(current_aspect.strip())\n",
        "                current_aspect = \"\"\n",
        "    if current_aspect:\n",
        "        aspects.append(current_aspect.strip())\n",
        "    return aspects\n",
        "\n",
        "# Analyze sentiment function\n",
        "def analyze_sentiment(aspect, text, model, tokenizer):\n",
        "    input_text = f\"{aspect}: {text}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.softmax(outputs.logits, dim=1)\n",
        "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
        "    sentiment = model.config.id2label[predicted_class]\n",
        "    confidence = predictions[0][predicted_class].item()\n",
        "    return sentiment, confidence\n",
        "\n",
        "# TEST EXAMPLES\n",
        "test_texts = [\n",
        "    \"The sushi was fresh and the drinks were cold.\",\n",
        "    \"Great food but terrible service.\",\n",
        "    \"The atmosphere was cozy but the prices were too high.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(\"=\"*60)\n",
        "    print(f\"TEXT: {text}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    aspects = extract_aspects(text, aspect_model, aspect_tokenizer)\n",
        "    print(f\"Found aspects: {aspects}\\n\")\n",
        "\n",
        "    for aspect in aspects:\n",
        "        sentiment, confidence = analyze_sentiment(aspect, text, sentiment_model, sentiment_tokenizer)\n",
        "        print(f\"  ‚Ä¢ {aspect:15} ‚Üí {sentiment:8} ({confidence:.1%})\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "aMG1Y5srglQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to see the problem:\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"tomaarsen/setfit-absa-semeval-restaurants\")\n",
        "\n",
        "label_counts = {}\n",
        "for item in dataset[\"train\"]:\n",
        "    label = item['label']\n",
        "    label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "print(\"Label distribution in dataset:\")\n",
        "for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  {label}: {count} ({count/len(dataset['train'])*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "yhIIVUGesIj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More test examples\n",
        "test_texts = [\n",
        "    # Mixed sentiments\n",
        "    \"The pasta was delicious but the waiter was rude.\",\n",
        "    \"Loved the ambiance, hated the music.\",\n",
        "    \"The steak was overcooked but the wine was excellent.\",\n",
        "\n",
        "    # All positive\n",
        "    \"Amazing pizza, friendly staff, and great location!\",\n",
        "    \"The dessert was heavenly and the coffee was perfect.\",\n",
        "\n",
        "    # All negative\n",
        "    \"Terrible food, slow service, and dirty tables.\",\n",
        "    \"The soup was cold and the bread was stale.\",\n",
        "\n",
        "    # Neutral-ish\n",
        "    \"The menu had many options and the restaurant was busy.\",\n",
        "\n",
        "    # Multi-word aspects\n",
        "    \"The fish tacos were fresh but the french fries were soggy.\",\n",
        "    \"The ice cream was amazing but the apple pie was disappointing.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(\"=\"*60)\n",
        "    print(f\"TEXT: {text}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    aspects = extract_aspects(text, aspect_model, aspect_tokenizer)\n",
        "    print(f\"Found aspects: {aspects}\\n\")\n",
        "\n",
        "    for aspect in aspects:\n",
        "        sentiment, confidence = analyze_sentiment(aspect, text, sentiment_model, sentiment_tokenizer)\n",
        "        print(f\"  ‚Ä¢ {aspect:15} ‚Üí {sentiment:8} ({confidence:.1%})\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "9O6AkkebyUqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "\n",
        "# Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "bxCzb0Jk1rLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Replace with your Hugging Face username!\n",
        "USERNAME = \"AnasAhmadz\"  # ‚ö†Ô∏è CHANGE THIS!\n",
        "\n",
        "# Upload Aspect Model\n",
        "print(\"Uploading Aspect Extraction Model...\")\n",
        "aspect_model.push_to_hub(f\"{USERNAME}/aspect-extraction-bert\")\n",
        "aspect_tokenizer.push_to_hub(f\"{USERNAME}/aspect-extraction-bert\")\n",
        "print(\"‚úì Aspect model uploaded!\")\n",
        "\n",
        "# Upload Sentiment Model\n",
        "print(\"\\nUploading Sentiment Model...\")\n",
        "sentiment_model.push_to_hub(f\"{USERNAME}/aspect-sentiment-bert\")\n",
        "sentiment_tokenizer.push_to_hub(f\"{USERNAME}/aspect-sentiment-bert\")\n",
        "print(\"‚úì Sentiment model uploaded!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ BOTH MODELS UPLOADED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Aspect Model: https://huggingface.co/{USERNAME}/aspect-extraction-bert\")\n",
        "print(f\"Sentiment Model: https://huggingface.co/{USERNAME}/aspect-sentiment-bert\")"
      ],
      "metadata": {
        "id": "dBJ3pm-r133g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}